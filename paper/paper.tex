\documentclass[12pt,a4paper]{article}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amscd}
\usepackage{booktabs}
\usepackage[latin2]{inputenc}
\usepackage{t1enc}
\usepackage[mathscr]{eucal}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{graphics}
\numberwithin{equation}{section}
\usepackage{hyperref}
\usepackage[margin=2.9cm]{geometry}
\usepackage[outdir=./images/]{epstopdf}
\epstopdfsetup{outdir=./images/}
\usepackage{algorithm}
\usepackage{algpseudocode}

\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}

\def\numset#1{{\\mathbb #1}}

\begin{document}
 
\title{Estimating Gaze Duration Error Distributions from Eye Tracking Data}

\author{
John Hawkins \\ email \href{mailto:john.hawkins@playgroundxyz.com}{john.hawkins@playgroundxyz.com} \\
} 

\maketitle

\section{Abstract}

Eye tracking applications produce a series of gaze fixation points that can be attributed to
objects within a subject's field of vision. Error is typically measured on the basis of individual
gaze fixation point measurements. However, often these applications are used to infer a gaze duration
metric from a series of fixation measurements. In this work we develop an algorithm for estimating
the error bounds on gaze duration measurement through monte carlo simualtion based on the content
of an eye tracking validation log file. We provide this algorithm as an open source application
that allows for robust estimates of the error distribution of gaze duration measurements. 
We use this application to conduct experiments on the expected error bounds for different duration
measurements across a range of session lengths and area of interest positions within the viewport
of mobile devices. The results indicate...

\section{Introduction}

Eye tracking is an important technology with a wide variety of applications. It permits evaluation
of software user interfaces\cite{Harezlak2015}, the development of new forms of software interaction, as well as a potential method of biometric identification\cite{Kasprowski2018}. 
Eye tracking has become a core tool for empirical investigations 
into human behaviour and is widely used as a method of measuring explicit attention to visual stimuli. 
It has been applied to study psychological phenomena ranging from cognition\cite{Brunye2019}
to mental health\cite{Duque2014,Rudich-Strassler2022}, and is now routinely used to evaluate 
advertising \cite{Hervet2011}. Eye tracking technology has allowed marketing researchers 
to study many factors that contribute to effective advertising, 
including brand recall \cite{Wedel2000},
the capture and tranfer of attention \cite{Pieters2004}, 
the impact of images of faces \cite{Djamasbi2010},
the attention effects of animation \cite{Hamborg2012},
and the relationship with social media posts \cite{Barreto2013}.

Increasingly, eye tracking applications are built using machine learning models trained on 
data sets containing combinations of facial images and fixation coordinates collected 
through careful application design. The resulting gaze fixation prediction model can 
then be applied sequentially to determine how long a subject fixated on a given object.
Thus, gaze duration is derived from aggregation of gaze fixation.

Gaze fixation models will typically have an error profile that is dependent on both 
the nature of the media being presented, the variety of faces and lighting conditions 
in the training data. In addition, it has been observed that the error profile varies 
depending on the location of the true fixation point within the subjects field of view.
The errors in fixation point measurement for a given eye tracking solution are routinely
adjusted through a process called calibration. Improvements in the callibration process
are an ongoing focus for the development of algorithms\cite{Zhang2014,Hassoumi2019} 
and software tools\cite{ETCAL2018} 

When fixation models are used sequentially for the purpose of estimating gaze duration
(as is commonly done in point of salience or advertising media studies), 
then there is the potential for the error
to either cancel out (reducing error in gaze duration estimation) or to compound
(increasing error in gaze duration). Which of these outcomes occurs will depend on the
specifics of the model and the conditions under which it is being used.
Influencing factors include the ratio of measured gaze duration to the total viewing time
and the error profile of the machine learning fixation coordinate model.

The errors of eye tracking data technology can be decomposed into a range of
independent sources that researchers need to consider \cite{Holmqvist2012}.
Unless they are properly addressed these sources of error can manifest themselves
as systematic biases across undesirable dimensions such as the age\cite{Dalrymple2018}
or ethnicity of subjects\cite{Blignaut2013}. 

The use of eye fixation models to estimate gaze duration (or dwell time) in Area of
Interest studies (AOI) was discussed by Holmqvist et al. \cite{Holmqvist2012}. The
authors simulated the impact of gaze fixation error by adding noise according to
manufacturer specifications to data sets of low margin areas of interest. We extend 
this idea to calculate probabilistic bounds on the error in gaze duration using
calibration data as a source of noise distribution that is specific to both the
study participants and the device/environment of the study. The result is an open
source application that may be used by a wide variety of reserachers to provide
error bounds on any gaze duration measurements.


\section{Methodology}

We estimate the gaze duration measurement error through Monte Carlo simulation
on the basis of the gaze fixation errors on the validation data from eye tracking
fixation model. This error profile can be stratified into a grid across the screen dimensions. 
For each of these grid locations we calculate two error probabilities, the false negative 
probability (when true fixation is on the grid and the model produces coordinates outside the grid. 
And the false positive probability, when the true fixation is outside the grid location and the model produces a prediction inside the grid square.

We apply this grid of error probabilities in a Monte Carlo simulation that requires the following inputs:
1.	The grid coordinates in which the item of concern is located.
2.	The duration of true fixation on that item
3.	The duration of the entire session of measurement.

The Monte Carlo process will then repeatedly simulate sessions of the given length and gaze duration. Each simulation will use the error profiles to produce a potential model output. Collectively these estimates model outputs constitute a probability distribution over outputs for the given scenario.

\subsection{Measurement Distribution}

In the first stage of the methodology we provide an estimate of the distribution of measured
durations for all possible true gaze durations. These distributions are produced by generating
random fixation paths across the available screen dimensions, and adding noise that is 
drawn from the calibration data to make it consistent with the observed properties of the gaze model.
This produces a set of distributions that capture the expected
variation in measurement depending on the length of true fixation.

Formally stated this is the probability distribution over measured durations $\hat{d}$ given a
known duration $d$, the session length $s$ and the gaze target location $l$. 
We express this distribution as $P(\hat{d}|d,l,s)$. The
procedure for making this estimation involves iteratively generating random samples
of gaze fixation paths $\mathcal{F}$ consistent with the parameters $d$, $s$ and $l$. 
For each point $f$ in $\mathcal{F}$ we determine a measurement point $\hat{f}$ 
by drawing samples from the eye tracking validation set. 
These points determine the $\delta_x$ and $\delta_y$ error in the measurement
of gaze fixation point $f$. We draw these coordinates from the Calibration file such the
the probability of the point being chosen is propotional to the distance of its target
$x$ and $y$ from the coordinates of the point $f$ in the generated path. The complete algorithm
for estimating $P(\hat{d}|d,l,s)$ is shown in Algorithm \ref{alg:p_of_dhat}    

\begin{algorithm}
\caption{Estimation of $P(\hat{d}|d,l,s)$}\label{alg:p_of_dhat}
\begin{algorithmic}
  \Require $l,s,C$
  \Ensure $P(\hat{d}|d,l,s)$
  \State $N \gets samples$              \Comment{Configure simulation samples per duration}
  \State $D \gets floor(s / increment)$ \Comment{Durations are sampled at discrete intervals}
  \State $P \gets Array(D+1,D+1)$       \Comment{The distribution P is a 2 dimensional array}
  \For{$d \in 0 to D$}                  \Comment{Iterate over all possible true gaze durations}
    \For{$n \in 0 to N$}                \Comment{Iterature to collect the N samples for d}
      \State $p \gets generatePath(d,l,s)$
      \State $d_e \gets generateMeasurement(p,C)$  
      \State $P[d,d_e] += 1/N$          \Comment{Probability increment for a measuremnt of $d_e$}
    \EndFor
  \EndFor
  \State return P
\end{algorithmic}
\end{algorithm}


The bulk of the work is done by two functions inside the simulation loop. The first function
$generatePath(d,l,s)$ takes a true gaze duration (for that simulation iteration) the location
of the gaze target and the viewing session length. It then
generates a random path of fixation points that is compatible with those parameters.
The second function 
$generateMeasurement(p,l,C)$, takes the path, target location and the eye tracking calibration 
data and produces a sample of measured attention time. It does this by using the calibration data to sample
fixation errors and apply them to the path data. The noisy path data is then used to determine
the estimated gaze duration by looking at the number of fixation points that intersect with the target
location.
Repeated application of this process for a given value of $d$ produces multiple samples of $\hat{d}$
for that true gaze duration. Repeating the process over different 
values of $d$ allows us to populate a two dimensional array where the first dimension is the
true duration and the second the estimated duration. Note that our probability distribution is discrete
because we restrict ourselves to sampling every increment point between zero and the session length.

\subsection{Gaze Duration Distribution}

In real world applications we will have the true session length but rely on the model for the 
estimate of gaze duration. Meaning that the probability distribution we want is the distribution 
over true gaze durations for a given measurement, rather than the inverse which is what we 
have produced in the previous section. We produce an estimate of $P(d|\hat{d},l,s)$
through an application of Bayes' rule, as shown in Equation \ref{eq:p_of_d}.

\begin{equation}
\label{eq:p_of_d}
P(d|\hat{d},l,s) =  \frac{ P(\hat{d},l,s|d) \dot P(d) }{ P(\hat{d},l,s)  }
\end{equation}

We use a uniform prior for $P(d)$, meaning in the absence of additional information all
gaze durations less than the session length are equally likely. As our distributions are
discrete estimations of an underlying continuous distribution the value of $P(d)$ is
equal to $1/(1+floor(10*s))$
 We can estimate the value
of the denominator $P(\hat{d},l,s)$ by iterating over all values of d and summing the product
of $ P(\hat{d},l,s|d) \dot P(d)$. This fully explicated form is shown in 
Equation \ref{eq:p_of_d_full}.

\begin{equation}
\label{eq:p_of_d_full}
P(d|\hat{d},l,s) =  \frac{ P(\hat{d},l,s|d) \dot P(d) }{ \sum_{\delta \in D} P(\hat{d},l,s|\delta) \dot P(\delta)  }
\end{equation}

\subsection{Experiments}

We evaluate the methodology by conducting experiments over a range of AOI positions and session
lengths. The goal of which is to understand changes in the expected error profile of 
visual attention measurement when the object position varies and under variations in the
length of time that a subject is tracked.

In both instances we use an identical validation dataset generated from a panel of 15 users
engaged in a validation task after calibration with the SeeSo eye tracking software on mobile
devices. In these experiments we have used all validation data as single source of error 
estimation, but in principle these experiments could be conducted on a per user basis.

\section{Results}

We show all experimental results in Figure \ref{fig:plots}. The top series corresponds
to experiments where the AOI is the top third of the viewport, the middle and bottom 
series correspond to their respective thirds of the viewport. In each plot we show
data for a specific session length ranging from 10 to 30 seconds, where the x axis
depicts the measured gaze duration and the y-axis shows the confidence range for the
true gaze duration.  


\section{Conclusion}

Not sure yet.

\bibliographystyle{IEEEtran}
\bibliography{refs}

\end{document}
